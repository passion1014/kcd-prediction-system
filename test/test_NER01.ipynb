{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2ec8ec",
   "metadata": {},
   "source": [
    "# NER 모델 생성 테스트\n",
    "\n",
    "Hugging Face Transformers 라이브러리와 KoELECTRA 모델을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f128204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치 필요한 라이브러리\n",
    "# !pip install torch transformers scikit-learn seqeval\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3db721",
   "metadata": {},
   "source": [
    "# 라벨 정의\n",
    "\n",
    "## 핵심 라벨 (KCD분류의 뼈대)\n",
    "\n",
    "- 신체부위 (BODY) : 질병/상해 발생 위치\n",
    "    ex) 머리, 허리, 좌측 무릎, L4-5번 척추\n",
    "\n",
    "- 주진단 (DIS-MAIN) : 의학적으로 확정된 병명\n",
    "    ex) 골절, 염좌, 위염, 디스크, 뇌진탕\n",
    "\n",
    "- 증상 (SYMPTOM) : 진단명이 나오기 전, 환자가 느끼는 주관적 고통(R코드 예측에 중요)\n",
    "    ex) 호흡곤란, 통증, 불편함, 어지러움\n",
    "\n",
    "\n",
    "## 맥락 라벨 (상해/질병 구분용)\n",
    "\n",
    "- 사고원인 (CAUSE) : 상해 예측의 핵심. 외력에 의한 것인지 파악.\n",
    "    ex) 교통사고, 낙상, 미끄러짐, 접촉사고, 부딪힘\n",
    "\n",
    "- 행동/상황 (ACT) : 상해 발생 상황 파악. 사고 당시 무엇을 하고 있었는가? (산재 여부나 상해 기전 파악)\n",
    "    ex) 운전 중, 작업 중, 보행 중, 축구 하다가\n",
    "\n",
    "- 시점/기간 (TIME) : 금성인지 만성인지 구분.\n",
    "    ex) 갑자기, 예전부터, 만성적인, 3일전부터\n",
    "\n",
    "\n",
    "## 심화 라벨 (세부 코드 결정용)\n",
    "\n",
    "- 치료/수술 (TREATMENT) : 수술 여부는 중증도(코드의 앞자리)를 암시함.\n",
    "    ex) 봉합(열상), 핀삽입(골절), 물리치료, 입원\n",
    "\n",
    "- 방향/측면 (SIDE) : 좌/우/양측 (최근 KCD는 방향에 따라 코드가 갈림)\n",
    "    ex) 좌측, 우측, 양측\n",
    "\n",
    "- 과거력 (DIS-HIST) : 이번 사고/청구와 직접 관련 없는 과거의 병력이나 기저 질환.\n",
    "    ex) (허리)수술 이력, (고혈압)약 복용 중, 예전에 다친 적 있음, 지병\n",
    "\n",
    "- 부정어 (NEG) : 부정어가 포함된 경우. 골절 아님을 골절로 인식하면 안됨\n",
    "    ex) (골절)없음, 아님, (이상)소견 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d5aa8",
   "metadata": {},
   "source": [
    "\n",
    "# 라벨링 하기\n",
    "\n",
    "1차 라벨링은 Doccano와 ChatGPT를 사용한다.\n",
    "라벨링 JSON 포맷은 아래와 같다.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"이순신은 서울에서 거북선을 만들었다.\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"text\": \"이순신\",\n",
    "            \"label\": \"PER\",\n",
    "            \"start_offset\": 0,\n",
    "            \"end_offset\": 3\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"서울\",\n",
    "            \"label\": \"CITY\",\n",
    "            \"start_offset\": 5,\n",
    "            \"end_offset\": 7\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"거북선\",\n",
    "            \"label\": \"ARTIFACT\",\n",
    "            \"start_offset\": 10,\n",
    "            \"end_offset\": 13\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb1c35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 데이터 개수: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 학습 시작 (Dummy Data)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/passion1014/project/axlrator/ml/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 학습 완료!\n",
      "\n",
      ">>> [테스트 문장]: 위식도 역류성 질환으로 통원 치료를 받던 중 호흡곤란으로 대장내시경 검사를 받았는데 용종이 커져서 영향을 받은 것으로 용종점막절제술을 받게 된 것입니다.\n",
      "\n",
      ">>> [NER 추출 결과]\n",
      "Entity: 를  |  Label: CAUSE\n",
      "Entity: 으로  |  Label: CAUSE\n",
      "Entity: 를  |  Label: CAUSE\n",
      "Entity: 서  |  Label: CAUSE\n",
      "Entity: 을  |  Label: CAUSE\n",
      "Entity: 으로  |  Label: CAUSE\n",
      "Entity: 을  |  Label: CAUSE\n",
      "Entity: .  |  Label: CAUSE\n",
      "\n",
      ">>> [KCD 매핑을 위한 최종 구조]\n",
      "{'주진단(MAIN)': [], '부위(BODY)': [], '증상(SYMPTOM)': [], '과거력(HIST)': [], '수술(TREATMENT)': []}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 설정 및 태그 정의 (Configuration)\n",
    "# ---------------------------------------------------------\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\" # 한국어 성능이 우수한 모델\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 우리가 정의한 태그 리스트 (BIO Scheme)\n",
    "# B: 시작, I: 중간, O: 관련없음\n",
    "TAGS = [\n",
    "    \"O\",\n",
    "    \"B-DIS-MAIN\", \"I-DIS-MAIN\",   # 주진단 (예: 용종, 골절)\n",
    "    \"B-DIS-HIST\", \"I-DIS-HIST\",   # 과거력 (예: 위식도 역류성 질환)\n",
    "    \"B-SYMPTOM\", \"I-SYMPTOM\",     # 증상 (예: 호흡곤란, 통증)\n",
    "    \"B-BODY\", \"I-BODY\",           # 신체부위 (예: 대장, 손)\n",
    "    \"B-TREATMENT\", \"I-TREATMENT\", # 치료/수술 (예: 용종점막절제술)\n",
    "    \"B-CAUSE\", \"I-CAUSE\"          # 사고원인 (예: 낙하물)\n",
    "]\n",
    "\n",
    "# 태그 <-> ID 매핑 생성\n",
    "label2id = {tag: i for i, tag in enumerate(TAGS)}\n",
    "id2label = {i: tag for i, tag in enumerate(TAGS)}\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 데이터셋 클래스 정의 (Dataset)\n",
    "# ---------------------------------------------------------\n",
    "class MedicalNERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label2id, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        # 실제 학습시는 레이블링 툴(Doccano 등)에서 뽑은 정답 리스트가 들어와야 함\n",
    "        # 여기서는 데모를 위해 텍스트만 처리하거나, 더미 레이블을 매핑하는 로직이 필요\n",
    "        # *주의*: 실제 학습 데이터는 문장과 함께 [O, O, B-DIS, ...] 형태의 라벨 리스트가 있어야 함\n",
    "        \n",
    "        # 토크나이징 (Subword 단위 분리)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 데모용: 실제 정답 라벨이 있다고 가정하고 텐서 변환\n",
    "        # (실제 프로젝트에선 여기서 subword align 로직이 복잡하게 들어감)\n",
    "        labels = item.get('labels', [0] * self.max_len) \n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [데이터 생성 도우미 함수]\n",
    "# 문장과 태깅할 단어 리스트를 주면, 모델용 포맷(labels)으로 변환해줍니다.\n",
    "# ---------------------------------------------------------\n",
    "def make_training_data(tokenizer, text, entities, max_len):\n",
    "    \"\"\"\n",
    "    text: 원본 문장\n",
    "    entities: {\"단어\": \"태그명\"} 형태의 딕셔너리\n",
    "    max_len: 최대 길이\n",
    "    \"\"\"\n",
    "    # 1. 토크나이징 (Offset Mapping 포함: 토큰이 원문의 몇 번째 글자인지 위치 정보 반환)\n",
    "    tokenized = tokenizer(\n",
    "        text, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_len, \n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    # 2. 라벨 리스트 초기화 (모두 'O'(=0)으로 시작)\n",
    "    labels = [label2id[\"O\"]] * max_len\n",
    "    offsets = tokenized['offset_mapping']\n",
    "    \n",
    "    # 3. 각 Entity 위치 찾아서 라벨링 (BIO 태깅)\n",
    "    for word, tag_name in entities.items():\n",
    "        # 문장에서 단어의 시작 위치 찾기\n",
    "        start_char = text.find(word)\n",
    "        if start_char == -1: continue # 단어가 없으면 스킵\n",
    "        end_char = start_char + len(word)\n",
    "        \n",
    "        # 각 토큰이 이 단어 범위 안에 있는지 확인\n",
    "        # 예: '위식도'(0~3) -> 토큰 '위'(0~1), '식도'(1~3)\n",
    "        found_start = False\n",
    "        for idx, (offset_start, offset_end) in enumerate(offsets):\n",
    "            if offset_start == 0 and offset_end == 0: continue # 특수토큰 스킵\n",
    "            \n",
    "            # 토큰이 단어 범위 내에 완전히 포함되면\n",
    "            if offset_start >= start_char and offset_end <= end_char:\n",
    "                if not found_start:\n",
    "                    # 첫 토큰은 B-태그\n",
    "                    labels[idx] = label2id[f\"B-{tag_name}\"]\n",
    "                    found_start = True\n",
    "                else:\n",
    "                    # 나머지 토큰은 I-태그\n",
    "                    labels[idx] = label2id[f\"I-{tag_name}\"]\n",
    "                    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [사용 예시] - 이렇게 데이터를 추가하세요!\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "# 데이터 1\n",
    "data1 = make_training_data(\n",
    "    tokenizer,\n",
    "    text=\"위식도 역류성 질환으로 통원 치료를 받았다.\",\n",
    "    entities={\n",
    "        \"위식도 역류성 질환\": \"DIS-MAIN\", # 주진단\n",
    "        \"통원 치료\": \"TREATMENT\"          # 치료\n",
    "    },\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "training_data.append(data1)\n",
    "\n",
    "# 데이터 2\n",
    "data2 = make_training_data(\n",
    "    tokenizer,\n",
    "    text=\"호흡곤란으로 응급실 내원하여 폐렴 진단받음\",\n",
    "    entities={\n",
    "        \"호흡곤란\": \"SYMPTOM\",\n",
    "        \"폐렴\": \"DIS-MAIN\"\n",
    "    },\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "training_data.append(data2)\n",
    "\n",
    "# ... 계속 추가 ...\n",
    "{\n",
    "  \"text\": \"눈 통증으로 진료\",\n",
    "  \"entities\": {\n",
    "    \"눈\": \"BODY\",\n",
    "    \"통증\": \"SYMPTOM\",\n",
    "    \"진료\": \"TREATMENT\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "print(f\"생성된 데이터 개수: {len(training_data)}\")\n",
    "train_dataset = MedicalNERDataset(training_data, tokenizer, label2id, MAX_LEN)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. 모델 초기화 및 학습 설정 (Training Setup)\n",
    "# ---------------------------------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(TAGS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,              # 에포크 수 (NER은 금방 과적합되므로 적게)\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-5,              # 미세조정용 낮은 학습률\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",              # 데모용이라 저장 안 함\n",
    "    use_cpu=False                    # GPU 있으면 False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    ")\n",
    "\n",
    "# 학습 시작 (더미 데이터라 금방 끝남)\n",
    "print(\">>> 학습 시작 (Dummy Data)...\")\n",
    "trainer.train()\n",
    "print(\">>> 학습 완료!\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. 추론 및 후처리 로직 (Inference & Post-processing)\n",
    "# ---------------------------------------------------------\n",
    "def predict_ner(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    입력 텍스트에서 Entity를 추출하여 보기 좋게 반환하는 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 입력 처리\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # 결과 디코딩\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
    "\n",
    "    # 결과 정리 (Subword 병합 및 Entity 그룹화)\n",
    "    entities = []\n",
    "    current_entity = {\"word\": \"\", \"label\": None}\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "            \n",
    "        # Subword 처리 ('##'으로 시작하는 토큰 병합)\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            # 이전 Entity 저장\n",
    "            if current_entity[\"word\"]:\n",
    "                entities.append(current_entity)\n",
    "            # 새 Entity 시작\n",
    "            current_entity = {\"word\": clean_token, \"label\": label[2:]} # \"B-\" 제거\n",
    "            \n",
    "        elif label.startswith(\"I-\") and current_entity[\"label\"] == label[2:]:\n",
    "            # 현재 Entity에 이어붙이기\n",
    "            current_entity[\"word\"] += clean_token\n",
    "            \n",
    "        else: # \"O\" 태그이거나 라벨이 끊긴 경우\n",
    "            if current_entity[\"word\"]:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = {\"word\": \"\", \"label\": None}\n",
    "                \n",
    "    if current_entity[\"word\"]:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. 실제 테스트 (Demo)\n",
    "# ---------------------------------------------------------\n",
    "# *참고*: 학습 데이터가 더미(0)라서 결과는 엉망이겠지만, 로직 흐름은 확인 가능\n",
    "test_text = \"위식도 역류성 질환으로 통원 치료를 받던 중 호흡곤란으로 대장내시경 검사를 받았는데 용종이 커져서 영향을 받은 것으로 용종점막절제술을 받게 된 것입니다.\"\n",
    "\n",
    "print(\"\\n>>> [테스트 문장]:\", test_text)\n",
    "result = predict_ner(test_text, model, tokenizer)\n",
    "\n",
    "print(\"\\n>>> [NER 추출 결과]\")\n",
    "for entity in result:\n",
    "    print(f\"Entity: {entity['word']}  |  Label: {entity['label']}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. KCD 코드 매핑용 데이터 구조화 (Tip)\n",
    "# ---------------------------------------------------------\n",
    "final_structure = {\n",
    "    \"주진단(MAIN)\": [],\n",
    "    \"부위(BODY)\": [],\n",
    "    \"증상(SYMPTOM)\": [],\n",
    "    \"과거력(HIST)\": [],\n",
    "    \"수술(TREATMENT)\": []\n",
    "}\n",
    "\n",
    "for item in result:\n",
    "    tag = item['label']\n",
    "    word = item['word']\n",
    "    \n",
    "    if tag == \"DIS-MAIN\": final_structure[\"주진단(MAIN)\"].append(word)\n",
    "    elif tag == \"BODY\": final_structure[\"부위(BODY)\"].append(word)\n",
    "    elif tag == \"SYMPTOM\": final_structure[\"증상(SYMPTOM)\"].append(word)\n",
    "    elif tag == \"DIS-HIST\": final_structure[\"과거력(HIST)\"].append(word)\n",
    "    elif tag == \"TREATMENT\": final_structure[\"수술(TREATMENT)\"].append(word)\n",
    "\n",
    "print(\"\\n>>> [KCD 매핑을 위한 최종 구조]\")\n",
    "print(final_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6622d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 그대로: ['위', '##식', '##도', '##역', '##류', '##질', '##환', '##은', '통증', '##이', '심하', '##다']\n",
      "형태소 처리: 위 식도 역류 질환 은 통증 이 심하 다\n",
      "처리 후 토큰: ['위', '식도', '역류', '질환', '은', '통증', '이', '심하', '다']\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    형태소 분석기를 사용해 명사와 조사 사이를 강제로 띄어줍니다.\n",
    "    예: \"위식도질환은\" -> \"위식도 질환 은\"\n",
    "    \"\"\"\n",
    "    results = kiwi.analyze(text)\n",
    "    tokens = []\n",
    "    for token, pos, _, _ in results[0][0]:\n",
    "        # 필요하다면 여기서 불용어 제거나 특정 품사만 남길 수도 있음\n",
    "        tokens.append(token)\n",
    "    return \" \".join(tokens) # 띄어쓰기로 연결\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 비교 테스트\n",
    "# ---------------------------------------------------\n",
    "raw_text = \"위식도역류질환은 통증이 심하다\"\n",
    "\n",
    "# 1. 그냥 BERT 토크나이저\n",
    "print(f\"원본 그대로: {tokenizer.tokenize(raw_text)}\")\n",
    "# 결과 예시: ['위', '##식', '##도', '##역', '##류', '##질환은', ...] (조사가 붙을 수 있음)\n",
    "\n",
    "# 2. 형태소 분석 후 BERT 토크나이저\n",
    "preprocessed = preprocess_text(raw_text)\n",
    "print(f\"형태소 처리: {preprocessed}\")\n",
    "print(f\"처리 후 토큰: {tokenizer.tokenize(preprocessed)}\")\n",
    "# 결과 예시: \"위식도 역류 질환 은 통증 이 심하다\"\n",
    "# 토큰: ['위', '##식', '##도', '역류', '질환', '은', '통증', '이', ...] (조사가 확실히 떨어짐!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cae4b9",
   "metadata": {},
   "source": [
    "# 테스트 참고 사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc6af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위 (NNG)\n",
      "내시경 (NNG)\n",
      "검사 (NNG)\n",
      "를 (JKO)\n",
      "받 (VV-R)\n",
      "었 (EP)\n",
      "습니다 (EF)\n",
      ". (SF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "text = \"위내시경 검사를 받았습니다.\"\n",
    "\n",
    "# analyze 함수로 분석\n",
    "result = kiwi.analyze(text, top_n=1)\n",
    "\n",
    "for token, tag, start, _len in result[0][0]:\n",
    "    print(f\"{token} ({tag})\")\n",
    "\n",
    "# [출력 결과]\n",
    "# 위내시경 (NNG) -> 일반 명사\n",
    "# 검사 (NNG)\n",
    "# 를 (JKO) -> 목적격 조사 (BERT 분리 대상!)\n",
    "# 받 (VV) -> 동사\n",
    "# 었습니다 (EF) -> 어미\n",
    "# . (SF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a87344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위 식도 역류 질환 은 통증 이 심하 ᆷ\n"
     ]
    }
   ],
   "source": [
    "def clean_text_with_kiwi(text, kiwi_model):\n",
    "    \"\"\"\n",
    "    1. 의료 용어 사전을 기반으로 형태소를 분석한다.\n",
    "    2. 조사(Josa)를 떼어내기 위해 공백을 삽입한다.\n",
    "    3. 정제된 텍스트를 반환한다.\n",
    "    \"\"\"\n",
    "    # 형태소 분석 결과 가져오기\n",
    "    tokens = kiwi_model.tokenize(text)\n",
    "    \n",
    "    # 형태소(form)만 공백으로 연결\n",
    "    # 예: [위식도, 역류, 질환, 은] -> \"위식도 역류 질환 은\"\n",
    "    cleaned_text = \" \".join([t.form for t in tokens])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# 사용\n",
    "raw_text = \"위식도역류질환은 통증이심함\"\n",
    "processed_text = clean_text_with_kiwi(raw_text, kiwi)\n",
    "\n",
    "# 이 processed_text를 BERT Tokenizer에 넣으면 성능 대폭 향상!\n",
    "print(processed_text) \n",
    "# \"위식도역류질환 은 통증 이 심하 ㅁ\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
